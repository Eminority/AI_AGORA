import time
import random
import requests
import os
import json
from dotenv import load_dotenv  # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

def load_api_keys():
    """
    í™˜ê²½ ë³€ìˆ˜ì—ì„œ AI API í‚¤ë¥¼ ë¡œë“œí•˜ëŠ” ê³µí†µ í•¨ìˆ˜.
    """
    api_key_json = json.loads(os.getenv("AI_API_KEY"))
    
    # í™˜ê²½ ë³€ìˆ˜ ê°’ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print(f"ğŸ” [DEBUG] AI_API_KEY í™˜ê²½ ë³€ìˆ˜ ê°’: {api_key_json}")

    if api_key_json:
        try:
            api_keys = api_key_json  # JSON ë³€í™˜
            print(f"âœ… [DEBUG] ë³€í™˜ëœ API í‚¤ ë”•ì…”ë„ˆë¦¬: {api_keys}")  # ë””ë²„ê¹… ì¶œë ¥
            return api_keys
        except json.JSONDecodeError:
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ 'AI_API_KEY'ê°€ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ 'AI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

class DebateDataProcessor:
    def __init__(self, api_keys: dict, max_results=5, headless=True):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ.
        
        Args:
            api_keys (dict): AI_Factoryì—ì„œ ì „ë‹¬ëœ API í‚¤ ë”•ì…”ë„ˆë¦¬
        """
        self.api_key = api_keys.get("GSE")  # AI_Factoryì—ì„œ ì „ë‹¬ë°›ì€ ê°’ ì‚¬ìš©
        self.max_results = max_results
        self.headless = headless
        if not self.api_key:
            raise KeyError("í™˜ê²½ ë³€ìˆ˜ì—ì„œ 'GSE' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.cx = os.getenv("CX")
        if not self.cx:
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ì—ì„œ 'CX' ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

        print(f"âœ… Google Custom Search API í‚¤ ë¡œë“œ ì™„ë£Œ: {self.api_key}")
        print(f"âœ… Google Custom Search CX ID ë¡œë“œ ì™„ë£Œ: {self.cx}")
        self.driver = self._init_driver()

    def _init_driver(self):
        """
        Selenium WebDriver ì´ˆê¸°í™”.

        Args:
            headless (bool): ë¸Œë¼ìš°ì € ì°½ì„ í‘œì‹œí• ì§€ ì—¬ë¶€.

        Returns:
            webdriver.Chrome: ì„¤ì •ëœ WebDriver ì¸ìŠ¤í„´ìŠ¤.
        """
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument("--headless")  # ì°½ ì—†ì´ ì‹¤í–‰
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=options)

    def search_articles(self, query):
        """
        Google Custom Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë¥¼ ê²€ìƒ‰.

        Args:
            query (str): ê²€ìƒ‰ì–´.

        Returns:
            list: ë‰´ìŠ¤ ê¸°ì‚¬ ì›ë³¸ URL ë¦¬ìŠ¤íŠ¸.
        """
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "q": f"{query} site:news.google.com",
            "cx": self.cx,
            "key": self.api_key,
            "num": self.max_results,
            "tbm": "nws"  # ë‰´ìŠ¤ ê²€ìƒ‰ ëª¨ë“œ
        }

        response = requests.get(url, params=params)
        if response.status_code != 200:
            print(f"Google ê²€ìƒ‰ API ì˜¤ë¥˜ ë°œìƒ: {response.json()}")
            return []

        data = response.json()
        return [item["link"] for item in data.get("items", [])]

    def _extract_real_url(self):
        """
        Google ë‰´ìŠ¤ì˜ ë¦¬ë””ë ‰ì…˜ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URLì„ ì¶”ì¶œ.

        Returns:
            str: ì‹¤ì œ ê¸°ì‚¬ URL.
        """
        try:
            time.sleep(random.uniform(1, 2))  # ìµœì†Œí•œì˜ ëŒ€ê¸°ì‹œê°„ ì ìš©
            article_link_element = self.driver.find_element(By.TAG_NAME, "a")  # ì²« ë²ˆì§¸ ë§í¬ ì°¾ê¸°
            real_url = article_link_element.get_attribute("href")  # ì‹¤ì œ ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸°
            return real_url
        except Exception as e:
            print(f"ë¦¬ë””ë ‰ì…˜ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _fetch_article_content(self, url):
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ í¬ë¡¤ë§.

        Args:
            url (str): ë‰´ìŠ¤ ê¸°ì‚¬ URL.

        Returns:
            str: ê¸°ì‚¬ ë³¸ë¬¸.
        """
        self.driver.get(url)
        time.sleep(random.uniform(1, 3))  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ìµœì í™”

        # Google ë‰´ìŠ¤ ë¦¬ë””ë ‰ì…˜ í˜ì´ì§€ì¸ì§€ í™•ì¸
        if "google.com" in self.driver.current_url and "news" in self.driver.current_url:
            real_url = self._extract_real_url()
            if real_url:
                self.driver.get(real_url)  # ì‹¤ì œ ê¸°ì‚¬ URLë¡œ ì´ë™
                time.sleep(random.uniform(1, 2))  # ê¸°ì‚¬ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ìµœì í™”

        # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ëª¨ë“  ì½˜í…ì¸ ê°€ ë¡œë”©ë˜ë„ë¡ í•¨
        self._scroll_down()

        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        # í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ ìµœëŒ€í•œ ë§ì€ ë‚´ìš© í¬ë¡¤ë§)
        paragraphs = soup.find_all("p")
        article_text = "\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        return article_text if article_text else "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    def _scroll_down(self):
        """
        Seleniumì„ ì´ìš©í•˜ì—¬ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤ ë‹¤ìš´í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ë¡œë”©.
        """
        last_height = self.driver.execute_script("return document.body.scrollHeight")

        for _ in range(3):  # ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì¤„ì´ê¸°
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))  # ëœë¤ ëŒ€ê¸°
            new_height = self.driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                break
            last_height = new_height

    def get_articles(self, topic, num_articles=None):
        """
        ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³  ë³¸ë¬¸ì„ í¬ë¡¤ë§.

        Args:
            topic (str): ê²€ìƒ‰ ì£¼ì œ.
            num_articles (int): ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜. (ê¸°ë³¸ê°’ì€ ì„¤ì •ëœ max_results ê°’)

        Returns:
            list: ê¸°ì‚¬ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸.
        """
        num_articles = num_articles if num_articles else self.max_results
        article_links = self.search_articles(topic)

        articles_data = []
        for link in article_links[:num_articles]:  # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ì²˜ë¦¬
            article_content = self._fetch_article_content(link)
            if article_content:
                articles_data.append({"content": article_content})

        return articles_data

    def quit_driver(self):
        """
        WebDriver ì¢…ë£Œ.
        """
        if self.driver:
            self.driver.quit()


# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    processor = DebateDataProcessor(max_results=3, headless=True)
    topic = "AI technology"
    
    articles = processor.get_articles(topic)

    # ê²°ê³¼ ì¶œë ¥
    for idx, article in enumerate(articles, 1):
        print(f"ê¸°ì‚¬ {idx}:\n{article['content']}...\n")  

    processor.quit_driver()
