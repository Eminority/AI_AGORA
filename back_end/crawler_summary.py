import json
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

class DebateDataProcessor:
    """
    í† ë¡  ì£¼ì œì— ëŒ€í•œ í¬ë¡¤ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤.
    """

    def __init__(self, headless=True):
        self.driver = self._init_driver(headless)
        self.crawled_data = []  # í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥

    def _init_driver(self, headless):
        """Selenium WebDriver ì´ˆê¸°í™”"""
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-software-rasterizer")  # WebGL ì˜¤ë¥˜ í•´ê²°
        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    def search_articles(self, query, num_results=3):
        """ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ê¸°ì‚¬ URLì„ ê²€ìƒ‰ """
        search_url = f"https://www.npr.org/search?query={query.replace(' ', '+')}"
        self.driver.get(search_url)
        time.sleep(3)

        articles = self.driver.find_elements(By.CSS_SELECTOR, "article h2 a")

        article_links = [article.get_attribute("href") for article in articles[:num_results]]
        article_links = [link for link in article_links if link.startswith("https://www.npr.org/")]

        if not article_links:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return article_links

    def get_article_content(self, url):
        """ ê¸°ì‚¬ URLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ì„ í¬ë¡¤ë§ """
        self.driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        title_element = soup.find("h1")
        title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"

        content_elements = soup.select("div[data-testid='storytext'] p")  
        if not content_elements:
            content_elements = soup.select("div.transcript p")

        content = "\n".join([p.text.strip() for p in content_elements]) if content_elements else "âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return {"title": title, "content": content, "url": url}

    def get_articles(self, topic, num_articles=3):
        """ ì£¼ì œì™€ ê´€ë ¨ëœ ê¸°ì‚¬ë“¤ì„ í¬ë¡¤ë§ """
        print(f"ğŸ” '{topic}'ì— ëŒ€í•œ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘...")
        article_links = self.search_articles(topic, num_articles)

        if not article_links:
            print("âŒ ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"âœ… {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë°œê²¬! í¬ë¡¤ë§ ì‹œì‘...")
        articles = [self.get_article_content(link) for link in article_links]

        # ë³¸ë¬¸ì´ ë¹„ì–´ ìˆëŠ” ê¸°ì‚¬ ì œì™¸
        return [article for article in articles if article["content"] != "âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

    # def generate_summary(self, topic, articles):
    #     """ í¬ë¡¤ë§í•œ ê¸°ì‚¬ ë‚´ìš©ì„ ìš”ì•½ """
    #     if not articles:
    #         return "ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨: í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."

    #     combined_text = " ".join([article["content"] for article in articles[:3]])
    #     prompt = f"Summarize the following text about {topic} in 300 words:\n{combined_text}"

    #     return self.llm.generate_text(prompt)




#     def generate_pos_neg(self, topic, summary):
#         """ì°¬ì„± ë° ë°˜ëŒ€ ì˜ê²¬ ìƒì„±"""
#         pos_prompt = f"Provide a pro argument for {topic} based on the following summary:\n{summary}"
#         neg_prompt = f"Provide a con argument for {topic} based on the following summary:\n{summary}"

#         pos = self.llm.generate_text(pos_prompt) or "No pro argument generated."
#         neg = self.llm.generate_text(neg_prompt) or "No con argument generated."

#         return pos, neg

#     def run_pipeline(self, topic):
#         """ì£¼ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ í›„ ìš”ì•½"""
#         articles = self.get_articles(topic)
#         if not articles:
#             return None

#         summary = self.generate_summary(topic, articles)
#         pos, neg = self.generate_pos_neg(topic, summary)

#         return {
#             "topic": topic,
#             "summary": summary,
#             "pos": pos,
#             "neg": neg,
#             "articles": articles
#         }


#     def close(self):
#         """ WebDriver ì¢…ë£Œ """
#         self.driver.quit()


# # âœ… ì‹¤í–‰ ì˜ˆì œ
# if __name__ == "__main__":
#     test_topic = input("í† ë¡  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
#     processor = DebateDataProcessor(model_name="mistral", headless=False)  # GUI ì°½ ë„ìš°ê¸°

#     try:
#         print(f"\nğŸ” [1] '{test_topic}'ì— ëŒ€í•œ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...")
#         crawled_data = processor.run_pipeline(test_topic)

#         if crawled_data:
#             print("\nâœ… [2] í¬ë¡¤ë§ ì„±ê³µ! ë°ì´í„° ì¶œë ¥:")
#             print(json.dumps(crawled_data, indent=4, ensure_ascii=False))
#         else:
#             print("\nâŒ [2] í¬ë¡¤ë§ ì‹¤íŒ¨: ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

#     except Exception as e:
#         print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

#     finally:
#         processor.close()  # WebDriver ì¢…ë£Œ
