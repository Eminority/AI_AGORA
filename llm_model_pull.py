import json
import requests
import subprocess

class OllamaRunner:
    def __init__(self, model_name="mistral", base_url="http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url

    def is_model_installed(self):
        """í˜„ì¬ ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì´ ìˆëŠ”ì§€ ê²€ì‚¬"""
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            return self.model_name in result.stdout
        except FileNotFoundError:
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

    def pull_model(self):
        """ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ (URLì—ì„œ ê°€ì ¸ì™€ì„œ ì„¤ì¹˜)"""
        print(f"ğŸ” '{self.model_name}' ëª¨ë¸ í™•ì¸ ì¤‘...")
        if self.is_model_installed():
            print(f"âœ… '{self.model_name}' ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë¨.")
            return True

        print(f"ğŸ“¥ '{self.model_name}' ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        url = f"{self.base_url}/api/pull"
        response = requests.post(url, json={"name": self.model_name})
        
        if response.status_code == 200:
            print(f"âœ… '{self.model_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            return True
        else:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.text}")
            return False

    def run_model_interactive(self):
        """Ollama ëª¨ë¸ì„ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰ ('ollama run <model>')"""
        if not self.pull_model():
            print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨!")
            return

        print(f"ğŸš€ '{self.model_name}' ëª¨ë¸ì„ ì‹¤í–‰ ì¤‘... ")
        subprocess.run(["ollama", "run", self.model_name])

    def generate_text(self, prompt):
        """í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ Ollama ëª¨ë¸ì´ ì‘ë‹µ"""
        if not self.pull_model():
            print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨!")
            return "Error: Model could not be loaded"

        url = f"{self.base_url}/api/generate"
        payload = {"model": self.model_name, "prompt": prompt}

        with requests.post(url, json=payload, stream=True) as response:
            if response.status_code != 200:
                return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {response.text}"

            generated_text = ""
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)  # ì—¬ê¸°ì„œ ë³€ê²½
                        if "response" in data:
                            generated_text += data["response"] + " "
                    except json.JSONDecodeError:
                        continue  # JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œ

            return generated_text.strip()

# # ì‹¤í–‰ ì˜ˆì œ
# if __name__ == "__main__":
#     model_name = 'mistral'
#     ollama = OllamaRunner(model_name=model_name)

#     #í„°ë¯¸ë„ ì§ì ‘ ì…ë ¥
#     # ollama.run_model_interactive()

#     #ì›í•˜ëŠ” ëŒ€í™” íŒŒì´ì¬ ì°½ì—ì„œ ì…ë ¥
#     prompt = 'Can you debate with another AI?' #ì´ ë¶€ë¶„ì— ì ì„ ë‚´ìš© ì…ë ¥
#     response = ollama.generate_text(prompt)
#     print(f"ğŸ“ {model_name} ì‘ë‹µ: {response}")