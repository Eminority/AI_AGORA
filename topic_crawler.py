from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import json

class NPRCrawler:
    """NPRì—ì„œ íŠ¹ì • ì£¼ì œì˜ ê¸°ì‚¬ ê²€ìƒ‰ ë° ë³¸ë¬¸ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, headless=True):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument("--headless")  # GUI ì—†ì´ ì‹¤í–‰
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")

        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    def search_articles(self, query, num_results=3):
        """ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  ìƒìœ„ ê¸°ì‚¬ URLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
        search_url = f"https://www.npr.org/search?query={query.replace(' ', '+')}"
        self.driver.get(search_url)
        time.sleep(3)

        # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
        articles = self.driver.find_elements(By.CSS_SELECTOR, "h2.title a")
        article_links = [article.get_attribute("href") for article in articles[:num_results]]

        return article_links

    def get_article_content(self, url):
        """ ê¸°ì‚¬ URLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ """
        self.driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        # âœ… ê¸°ì‚¬ ì œëª© ê°€ì ¸ì˜¤ê¸°
        title_element = soup.find("h1")
        title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"

        # âœ… ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
        content_elements = soup.select("div[data-testid='storytext'] p")  # ê¸°ë³¸ ë³¸ë¬¸
        if not content_elements:
            content_elements = soup.select("div.transcript p")  # Transcript ë‚´ ë³¸ë¬¸ í…ìŠ¤íŠ¸

        content = "\n".join([p.text.strip() for p in content_elements])

        if not content:
            content = "âŒ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return {
            "title": title,
            "content": content,
            "url": url
        }

    def get_top_articles(self, topic, num_articles=3):
        """ ì£¼ì œì— ëŒ€í•œ ìƒìœ„ ê¸°ì‚¬ë“¤ì„ í¬ë¡¤ë§í•˜ì—¬ ë°˜í™˜ """
        article_links = self.search_articles(topic, num_articles)
        articles = [self.get_article_content(link) for link in article_links]
        return [article for article in articles if article]

    def close(self):
        """ ë“œë¼ì´ë²„ ì¢…ë£Œ """
        self.driver.quit()


# âœ… ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    topic = input("í† ë¡  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    crawler = NPRCrawler(headless=False)  # GUI ì°½ ë„ìš°ê¸°
    articles = crawler.get_top_articles(topic)

    print("\nğŸ” í¬ë¡¤ë§ëœ ê¸°ì‚¬:")
    for idx, article in enumerate(articles, 1):
        print(f"\nğŸ“Œ {idx}. {article['title']}\n{article['content'][:500]}...")  # ê¸´ ë³¸ë¬¸ì€ ì•ë¶€ë¶„ë§Œ í‘œì‹œ

    crawler.close()
